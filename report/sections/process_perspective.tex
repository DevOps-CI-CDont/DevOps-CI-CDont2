\section{Process' Perspective}
% SECTION PURPOSE: In essence it has to be clear how code or other artifacts come from idea into the running system and everything that happens on the way.
\subsection{Developer interaction}
\subsection{Organization of team}
\subsection{Full stage \& tool description of CI/CD Chains}
\subsubsection{Continuous integration}
We deployed a continuous integration(CI) workflow\cite{DevOps-CI-CDont} that would serve as a quality gate when pushing new changes. The aim was to define a set of tests that if ever failed we would not want to publish those changes to production. As the main focus of Minitwit was to handle requests from the simulator our tests only related to the backend and no end-to-end tests exist.\\\\
When a push or pull-request is accepted into main a virtual machine(VM) is spun up containing environment variables needed for establishing a connection to the database, these secrets are fed through GitHub's built-in "github secrets"\cite{GitHub-Secrets}. Once the machine is ready it builds a test-specific dockerfile that mimics our real dockerfile but instead connects to a test database rather than production. We then also install Digital ocean's command line interface "Doctl"\cite{DigitalOcean-doctl}, and push our new image to the Digital Oceans Container registry.\\\\
With this job completed we now have our API running on our virtual machine and we can start testing. Testing happens in a new job so again we spin up a virtual machine that installs doctl but this time it instead pulls the image the other job uploaded and then runs the image before making a call to "go test" running all go tests defined in the repository. Finally, we also have a static code analysis job running that installs all dependencies before running a typescript linter, eslint, and go vet. If either of these steps fails the job fails and the workflow as a whole exists with a "Failure". 
\subsubsection{Continuous deployment}
To support frequent delivery of new versions of mini twist we also created a continuous deployment(CD) workflow on Github actions. The workflow runs any time the CI workflow completes and the conclusion is "Success", meaning that all jobs succeeded without fail. The workflow then starts a VM that builds the backend and frontend images (along with the necessary secrets stored in github secrets), before installing doctl and pushing the images to the container registry.\\\\
But just pushing them to the registry is not enough as we need to communicate to our digital ocean droplet that a change was made to one of the images. To achieve this we utilize "Watchtower"\cite{watchtower}. Watchtower monitors running docker containers and watches for any changes made to the images that initially started those containers, if any images are updated watchtower will restart the container using the new image. Watchtower is set up on our project to look for a change on any of our images every 60 seconds. The combination of the CD and watchtower means that any time we push code(that passes the quality gate defined in CI) to main it also updates our hosted production server.+  


\subsection{Organization of repository}
Monorepo. 
% describe the structure of of mono-repository 
\subsection{Applied branching strategy}
\subsection{Applied development process}
We used Github's built-in issue tracking system, eventually also using those issues inside a Kanban-like board via a Github "Project" for the Github organization we had. At the end, we had 4 columns in the board (Todo, In Progress, Done and Unprioritized). Unprioritized recommended all the tasks that we found would be realistic tasks for the project if it were to continue, that weren't prioritized for the course. 
\subsection{Monitoring}
% How do you monitor your systems and what precisely do you monitor?
Prometheus \& Grafana ... \\
DigitalOcean resource alerts and uptime (\url{http://cicdont.live/public})
\subsection{Logging}
% What do you log in your systems and how do you aggregate logs?
Standard out from all containers...
\subsection{Security assessment}
% Brief results

\subsection{Applied strategy for scaling and load balancing}
\subsection{AI assistants}
% Reflect how it supported/hindered your process.
We have used Github Copilot via its VSCode extension.
A couple of areas where it was memorably useful:
\begin{enumerate}
    \item writing out repetitive patterns in the API (eg. error handling in Go, checking for certain parameters/cookies).
    \item writing fetch requests from the frontend (it seems to understand well how to use the endpoints that are described in the same repository).
    \item writing utility functions (Copilot is especially effective if given a perfectly descriptive function name, even better if standard terminology is used)
\end{enumerate} 
One challenge of using Copilot with an unfamiliar language (such as Go was to all of us), is that it can be really hard to tell if a suggestion is correct. Even if something seems to work as intended, it is still important to understand the code. \\\\
Another "AI Assistant" we have used is ChatGPT. A couple of areas where it was memorably useful: 
\begin{enumerate}
    \item Suggesting and explaining nginx configurations
    \item Debugging CORS errors, suggesting fixes with middleware in Go backend.
    \item Debugging memory usage problems. 
    \item Generating commands for iptables configuration.
\end{enumerate}
The conversational design is quite nice to be able to "tweak" its suggestions. It often spits out large blocks of code that aren't quite what you want, but it can fix that if told what to tweak. \\
A downside is that it may \textit{hallucinate} commands, flags, and functions that seem very plausible and one may think "How great! that command is exactly what I want", and then it doesn't exist. \\
When asked to use the functionality of a library, it will tend to use it in the way that there is most content like on the internet - this means the newest "best practices" aren't as likely to be used as whatever is still most common in the training data.  